# -*- coding: utf-8 -*-
"""CS7643 Final Project RNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16O9ZtB7PdiVWU8VqVMtIz1nPYa8c2ixo
"""

# Commented out IPython magic to ensure Python compatibility.
from collections import OrderedDict, defaultdict

!pip3 install pickle5
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import numpy as np
import pandas as pd
import pickle5 as pickle
import matplotlib.pyplot as plt
import matplotlib.style as style
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader, ConcatDataset

from google.colab import drive

drive.mount("/content/drive")
# %cd '/content/drive/MyDrive/master/cs7643/project/'

class VanillaRNN(nn.Module):
    def __init__(self, input_size, output_size, hidden_dim, n_layers, dropout, bidirectional):
        super(VanillaRNN, self).__init__()

        self.hidden_dim = hidden_dim
        self.n_layers = n_layers

        self.rnn = nn.RNN(input_size, hidden_dim, num_layers=n_layers, batch_first=True, nonlinearity='relu', bidirectional=bidirectional, dropout=dropout)   
        multiplier = 2 if bidirectional else 1
        self.fc = nn.Sequential(
          nn.Linear(n_layers * hidden_dim * multiplier, output_size * 2),
          nn.BatchNorm1d(output_size * 2),
          nn.ReLU(),
          nn.Linear(output_size * 2, output_size),
      )
    
    def forward(self, x):
        N = len(x)
        _, hidden = self.rnn(x)
        hidden = hidden.permute(1,0,2).contiguous().view(N, -1)
        hidden = self.fc(hidden)
        return hidden

class VanillaRNNWithCNN(nn.Module):
    def __init__(self, input_size, output_size, hidden_dim, n_layers, dropout, bidirectional):
        super(VanillaRNNWithCNN, self).__init__()
        self.hidden_dim = hidden_dim
        self.n_layers = n_layers

        self.cnn_encoder = nn.Sequential(
            nn.Conv1d(input_size, 8, kernel_size=3, stride=1, padding=1, bias=True),
            nn.BatchNorm1d(8),
            nn.ReLU(inplace=True),
            nn.Conv1d(8, 16, kernel_size=3, stride=1, padding=1, bias=True),
            nn.BatchNorm1d(16),
            nn.ReLU(inplace=True),
            nn.Conv1d(16, 32, kernel_size=3, stride=1, padding=1, bias=True),
            nn.BatchNorm1d(32),
            nn.ReLU(inplace=True),
        )
        self.rnn = nn.RNN(32, hidden_dim, num_layers=n_layers, batch_first=True, bidirectional=bidirectional, nonlinearity='relu', dropout=dropout)   
        multiplier = 2 if bidirectional else 1
        self.fc = nn.Sequential(
          nn.Linear(n_layers * hidden_dim * multiplier, output_size * 2),
          nn.BatchNorm1d(output_size * 2),
          nn.ReLU(),
          nn.Linear(output_size * 2, output_size),
      )
    
    def forward(self, x):
      N = len(x)
      out = x.permute(0, 2, 1)
      out = self.cnn_encoder(out)
      out = out.permute(0, 2, 1)
      _, hidden = self.rnn(out)
      hidden = hidden.permute(1,0,2).contiguous().view(N, -1)
      hidden = self.fc(hidden)
      return hidden

class VanillaLSTM(nn.Module):
  def __init__(self, input_size, output_size, hidden_dim, n_layers, dropout, bidirectional):
        super(VanillaLSTM, self).__init__()

        self.hidden_dim = hidden_dim
        self.n_layers = n_layers
        self.rnn = nn.LSTM(input_size, hidden_dim, num_layers=n_layers, batch_first=True, bidirectional=bidirectional, dropout=dropout)   
        multiplier = 2 if bidirectional else 1
        self.fc = nn.Sequential(
          nn.Linear(n_layers * hidden_dim * multiplier, output_size * 2),
          nn.BatchNorm1d(output_size * 2),
          nn.ReLU(),
          nn.Linear(output_size * 2, output_size),
      )
    
  def forward(self, x):
      N = len(x)
      _, (hidden, _) = self.rnn(x)
      hidden = hidden.permute(1,0,2).contiguous().view(N, -1)
      hidden = self.fc(hidden)
      return hidden

class VanillaLSTMWithCNN(nn.Module):
  def __init__(self, input_size, output_size, hidden_dim, n_layers, dropout, bidirectional):
      super(VanillaLSTMWithCNN, self).__init__()

      self.hidden_dim = hidden_dim
      self.n_layers = n_layers

      self.cnn_encoder = nn.Sequential(
          nn.Conv1d(input_size, 8, kernel_size=3, stride=1, padding=1, bias=True),
          nn.BatchNorm1d(8),
          nn.ReLU(inplace=True),
          nn.Conv1d(8, 16, kernel_size=3, stride=1, padding=1, bias=True),
          nn.BatchNorm1d(16),
          nn.ReLU(inplace=True),
          nn.Conv1d(16, 32, kernel_size=3, stride=1, padding=1, bias=True),
          nn.BatchNorm1d(32),
          nn.ReLU(inplace=True),
      )
      self.rnn = nn.LSTM(32, hidden_dim, num_layers=n_layers, batch_first=True, bidirectional=bidirectional, dropout=dropout)   
      multiplier = 2 if bidirectional else 1
      self.fc = nn.Sequential(
          nn.Linear(n_layers * hidden_dim * multiplier, output_size * 2),
          nn.BatchNorm1d(output_size * 2),
          nn.ReLU(),
          nn.Linear(output_size * 2, output_size),
      )
  def forward(self, x):
      N = len(x)
      out = x.permute(0, 2, 1)
      out = self.cnn_encoder(out)
      out = out.permute(0, 2, 1)
      _, (hidden, _) = self.rnn(out)
      hidden = hidden.permute(1,0,2).contiguous().view(N, -1)
      hidden = self.fc(hidden)
      return hidden

class VanillaGRU(nn.Module):
  def __init__(self, input_size, output_size, hidden_dim, n_layers, dropout, bidirectional):
        super(VanillaGRU, self).__init__()

        self.hidden_dim = hidden_dim
        self.n_layers = n_layers
        self.rnn = nn.GRU(input_size, hidden_dim, num_layers=n_layers, batch_first=True, bidirectional=bidirectional, dropout=dropout)   
        multiplier = 2 if bidirectional else 1
        self.fc = nn.Sequential(
            nn.Linear(n_layers * hidden_dim * multiplier, output_size * 2),
            nn.BatchNorm1d(output_size * 2),
            nn.ReLU(),
            nn.Linear(output_size * 2, output_size),
        )
    
  def forward(self, x):
      N = len(x)
      _, hidden = self.rnn(x)
      hidden = hidden.permute(1,0,2).contiguous().view(N, -1)
      hidden = self.fc(hidden)
      return hidden

class VanillaGRUWithCNN(nn.Module):
  def __init__(self, input_size, output_size, hidden_dim, n_layers, dropout, bidirectional):
      super(VanillaGRUWithCNN, self).__init__()

      self.hidden_dim = hidden_dim
      self.n_layers = n_layers

      self.cnn_encoder = nn.Sequential(
          nn.Conv1d(input_size, 8, kernel_size=3, stride=1, padding=1, bias=True),
          nn.BatchNorm1d(8),
          nn.ReLU(inplace=True),
          nn.Conv1d(8, 16, kernel_size=3, stride=1, padding=1, bias=True),
          nn.BatchNorm1d(16),
          nn.ReLU(inplace=True),
          nn.Conv1d(16, 32, kernel_size=3, stride=1, padding=1, bias=True),
          nn.BatchNorm1d(32),
          nn.ReLU(inplace=True),
      )
      self.rnn = nn.GRU(32, hidden_dim, num_layers=n_layers, batch_first=True, bidirectional=bidirectional, dropout=dropout)   
      multiplier = 2 if bidirectional else 1
      self.fc = nn.Sequential(
          nn.Linear(n_layers * hidden_dim * multiplier, output_size * 2),
          nn.BatchNorm1d(output_size * 2),
          nn.ReLU(),
          nn.Linear(output_size * 2, output_size),
      )
  def forward(self, x):
      N = len(x)
      out = x.permute(0, 2, 1)
      out = self.cnn_encoder(out)
      out = out.permute(0, 2, 1)
      _, hidden = self.rnn(out)
      hidden = hidden.permute(1,0,2).contiguous().view(N, -1)
      hidden = self.fc(hidden)
      return hidden

# Util functions
def draw(drawing):
  for x,y in drawing:
    plt.plot(x, y, marker='.')
    plt.axis('off')
  plt.gca().invert_yaxis()
  plt.axis('equal')
  plt.show()

def one_hot(category):
    arr = np.zeros(len(categories))
    arr[category] = 1.0
    return arr

def train(model, X_train, y_train, epochs = 1, n_chunks = 1000, learning_rate = 0.003, weight_decay = 0, optimizer = 'SGD'):
  print("Training model with epochs = {epochs}, learning rate = {lr}\n".format(epochs = epochs, lr = learning_rate))

  criterion = nn.CrossEntropyLoss()

  if optimizer == 'SGD':
    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=weight_decay)
  else:
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)
  losses = []
  for epoch in range(epochs):
    running_loss = 0
    images = torch.chunk(X_train, n_chunks)
    labels = torch.chunk(y_train, n_chunks)

    N = len(images)

    for i in range(N):
      optimizer.zero_grad()
      output = model.forward(images[i])
      loss = criterion(output, labels[i])
      loss.backward()
      optimizer.step()

      running_loss += loss.item()
    losses.append(running_loss)
    print("Epoch: {}/{}... ".format(epoch + 1, epochs), "Loss: {:.4f}".format(running_loss))
  return losses
  
def get_prediction_probability_distribution(model, input):
  with torch.no_grad():
    logits = model.forward(input)
  return torch.nn.functional.softmax(logits, dim=1)

def get_labels(model, input):
  probability_distribution = get_prediction_probability_distribution(model, input)
  pred_np = probability_distribution.numpy()
  N = len(pred_np)
  pred_values = np.amax(pred_np, axis=1, keepdims=True)
  pred_labels = np.array([np.where(pred_np[i, :] == pred_values[i, :])[0] for i in range(pred_np.shape[0])])
  pred_labels = pred_labels.reshape(N, 1)
  return pred_labels

def evaluate_model(model, train, y_train, test, y_test):
  train_pred_labels = get_labels(model, train)
  test_pred_labels = get_labels(model, test)
  y_train_label = np.argmax(y_train,axis=1)
  y_test_label = np.argmax(y_test,axis=1)
  accuracy_train = accuracy_score(y_train_label, train_pred_labels)
  accuracy_test = accuracy_score(y_test_label, test_pred_labels)

  print("Accuracy score for train set is {} \n".format(accuracy_train))
  print("Accuracy score for test set is {} \n".format(accuracy_test))

  return accuracy_train, accuracy_test

is_cuda = torch.cuda.is_available()
if is_cuda:
    device = torch.device("cuda")
    print("GPU is available")
else:
    device = torch.device("cpu")
    print("GPU not available, CPU used")

categories = ['flower', 'apple', 'baseball', 'baskebtall', 'bird', 'book', 'bus', 'car', 'cat', 'dog']
label_dict = {0:'flower', 1:'apple', 2:'baseball', 3:'baskebtall', 4:'bird',
                      5:'book',6:'bus', 7:'car', 8:'cat', 9:'dog'}

# Data Hyperparameters
test_set_split = 0.2
MAX_SEQUENCE_LENGTH = 50
EOS_TOKEN = None
SEQUENCE_DIMENSION = 3 # [x, y, is_stroke_start]

class RNNDataset(Dataset):
    """Face Landmarks dataset."""

    def __init__(self, parsed_data, transform=None):
        """
        Args:
            parsed_data (string): Path to the parsed_data.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        with open(parsed_data, "rb") as fh:
          self.file_pkg = pickle.load(fh)
        # self.file_pkg = pd.read_pickle(parsed_data)
        self.transform = transform

    def __len__(self):
        return len(self.file_pkg)

    def __getitem__(self, idx):
        return self.file_pkg.iloc[idx].to_dict()

def collate_fn(batch):
  N = len(batch)
  for i in range(N):
    seq = batch[i]['drawing']
    category = batch[i]['class']
    input = np.zeros((MAX_SEQUENCE_LENGTH, SEQUENCE_DIMENSION), np.float32)
    input[0:len(seq)] = seq[: MAX_SEQUENCE_LENGTH]
    batch[i]['drawing'] = input
  return batch

# Returns a DataLoader
# It already shuffles data as per sampler option
def read_rnn_data(batch_size=4, num_workers=0):
	transformed_dataset = RNNDataset('./rnn_parsed_data.pkl')
	return DataLoader(transformed_dataset, batch_size, num_workers, collate_fn=collate_fn)

# Load data for each category
classes = defaultdict(lambda: [])

# read data
ENABLE_MULTI_BATCH = False
READ_BATCH_SIZE = 20000
loader = read_rnn_data(batch_size=READ_BATCH_SIZE, num_workers=10)
for i_batch, sample_batched in enumerate(loader):
  N = len(sample_batched)
  for i in range(N):
    seq = sample_batched[i]['drawing']
    category = sample_batched[i]['class']
    classes[category].append(seq)
  if not ENABLE_MULTI_BATCH:
    break

# for category in categories:
#     # data = pd.read_csv("../input/train_simplified/" + category + ".csv")
#     data = [[[1,1,0],[2,2,0],[3,3,1],[4,4,1],[5,5,1]], [[1,1,0],[2,2,0],[3,3,1],[4,4,1],[5,5,1]]] # dummy
#     N = len(data)
#     input = np.zeros((N, MAX_SEQUENCE_LENGTH, SEQUENCE_DIMENSION), np.float32)
#     for seq in data:
#       length, _ = np.shape(seq)
      
#     for i in range(N):
#       seq = data[i]
#       input[i, 0:len(seq)] = seq[: MAX_SEQUENCE_LENGTH]
#     classes[category] = input

# Process input output pairs
X = []
y = []
for key, value in label_dict.items():
    data_i = classes[value]
    for data in data_i:
      X.append(data)
      y.append(one_hot(key))

X = np.array(X)
y = np.array(y)

# Train test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# Convert data to torch
X_train = torch.from_numpy(X_train).float()
y_train = torch.from_numpy(y_train).float()
X_test = torch.from_numpy(X_test).float()
y_test = torch.from_numpy(y_test).float()

# Model Hyperparameters
input_size = 3
hidden_sizes = 16
output_size = len(categories)
n_layers = 4
dropout = 0.1
weight_decay = 0.0
epochs = 20
n_chunks = 200
learning_rate = 0.01
optimizer = 'SGD'
bidirectional= True

# Define model
model = VanillaRNN(input_size, output_size, hidden_sizes, n_layers, dropout, bidirectional)

# Train model
loss = train(model, X_train, y_train, epochs=epochs, learning_rate = learning_rate, weight_decay = weight_decay, n_chunks = n_chunks, optimizer = optimizer)

# get accuracy
accuracy_train, accuracy_test = evaluate_model(model, X_train, y_train, X_test, y_test)

# Model Hyperparameters
input_size = 3
hidden_sizes = 16
output_size = len(categories)
n_layers = 4
dropout = 0.0
weight_decay = 0.0
epochs = 20
n_chunks = 200
learning_rate = 0.01
optimizer = 'SGD'
bidirectional = True

# Define model
model = VanillaRNNWithCNN(input_size, output_size, hidden_sizes, n_layers, dropout, bidirectional)

# Train model
loss = train(model, X_train, y_train, epochs=epochs, learning_rate = learning_rate, weight_decay = weight_decay, n_chunks = n_chunks, optimizer = optimizer)

# get accuracy
accuracy_train, accuracy_test = evaluate_model(model, X_train, y_train, X_test, y_test)

# Model Hyperparameters
input_size = 3
hidden_sizes = 16
output_size = len(categories)
n_layers = 3
dropout = 0.0
weight_decay = 0.0
epochs = 20
n_chunks = 100
learning_rate = 0.1
optimizer = 'SGD'
bidirectional = True

# Define model
model = VanillaLSTM(input_size, output_size, hidden_sizes, n_layers, dropout, bidirectional)

# Train model
loss = train(model, X_train, y_train, epochs=epochs, learning_rate = learning_rate, weight_decay = weight_decay, n_chunks = n_chunks, optimizer = optimizer)

# get accuracy
accuracy_train, accuracy_test = evaluate_model(model, X_train, y_train, X_test, y_test)

# Model Hyperparameters
input_size = 3
hidden_sizes = 16
output_size = len(categories)
n_layers = 4
dropout = 0.05
weight_decay = 0.0
epochs = 10
n_chunks = 200
learning_rate = 0.1
optimizer = 'SGD'
bidirectional = True

# Define model
model = VanillaLSTMWithCNN(input_size, output_size, hidden_sizes, n_layers, dropout, bidirectional)

# Train model
loss = train(model, X_train, y_train, epochs=epochs, learning_rate = learning_rate, weight_decay = weight_decay, n_chunks = n_chunks, optimizer = optimizer)

# get accuracy
accuracy_train, accuracy_test = evaluate_model(model, X_train, y_train, X_test, y_test)

# Model Hyperparameters
input_size = 3
hidden_sizes = 16
output_size = len(categories)
n_layers = 3
dropout = 0.0
weight_decay = 0.0
epochs = 20
n_chunks = 100
learning_rate = 0.1
optimizer = 'SGD'
bidirectional = True

# Define model
model = VanillaGRU(input_size, output_size, hidden_sizes, n_layers, dropout, bidirectional)

# Train model
loss = train(model, X_train, y_train, epochs=epochs, learning_rate = learning_rate, weight_decay = weight_decay, n_chunks = n_chunks, optimizer = optimizer)

# get accuracy
accuracy_train, accuracy_test = evaluate_model(model, X_train, y_train, X_test, y_test)

# Model Hyperparameters
input_size = 3
hidden_sizes = 16
output_size = len(categories)
n_layers = 3
dropout = 0.0
weight_decay = 0.0
epochs = 20
n_chunks = 100
learning_rate = 0.1
optimizer = 'SGD'
bidirectional = True

# Define model
model = VanillaGRUWithCNN(input_size, output_size, hidden_sizes, n_layers, dropout, bidirectional)

# Train model
loss = train(model, X_train, y_train, epochs=epochs, learning_rate = learning_rate, weight_decay = weight_decay, n_chunks = n_chunks, optimizer = optimizer)

# get accuracy
accuracy_train, accuracy_test = evaluate_model(model, X_train, y_train, X_test, y_test)

# drawing = [[[121,107,45,17,1,0,4,21,58,118,173,197,209,224,244,254,254,209,164,124],[47,57,56,75,93,114,123,140,162,187,196,187,177,164,136,115,101,83,71,43]],[[123,126],[43,0]]]
# draw(drawing)

