{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gm5mcmHesUyJ",
        "outputId": "5b1ca848-35fe-48ad-870b-d1f95af77bf9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pickle5 in /usr/local/lib/python3.7/dist-packages (0.0.12)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive/master/cs7643/project\n"
          ]
        }
      ],
      "source": [
        "from collections import OrderedDict, defaultdict\n",
        "\n",
        "!pip3 install pickle5\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle5 as pickle\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.style as style\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\")\n",
        "%cd '/content/drive/MyDrive/master/cs7643/project/'       "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {
        "id": "37Z5vMoXII9p"
      },
      "outputs": [],
      "source": [
        "class VanillaRNN(nn.Module):\n",
        "    def __init__(self, input_size, output_size, hidden_dim, n_layers, dropout, bidirectional):\n",
        "        super(VanillaRNN, self).__init__()\n",
        "\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.rnn = nn.RNN(input_size, hidden_dim, num_layers=n_layers, batch_first=True, nonlinearity='relu', bidirectional=bidirectional, dropout=dropout)   \n",
        "        multiplier = 2 if bidirectional else 1\n",
        "        self.fc = nn.Sequential(\n",
        "          nn.Linear(n_layers * hidden_dim * multiplier, output_size * 2),\n",
        "          nn.BatchNorm1d(output_size * 2),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(output_size * 2, output_size),\n",
        "      )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        N = len(x)\n",
        "        _, hidden = self.rnn(x)\n",
        "        hidden = hidden.permute(1,0,2).contiguous().view(N, -1)\n",
        "        hidden = self.fc(hidden)\n",
        "        return hidden\n",
        "\n",
        "class VanillaRNNWithCNN(nn.Module):\n",
        "    def __init__(self, input_size, output_size, hidden_dim, n_layers, dropout, bidirectional):\n",
        "        super(VanillaRNNWithCNN, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.cnn_encoder = nn.Sequential(\n",
        "            nn.Conv1d(input_size, 8, kernel_size=3, stride=1, padding=1, bias=True),\n",
        "            nn.BatchNorm1d(8),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv1d(8, 16, kernel_size=3, stride=1, padding=1, bias=True),\n",
        "            nn.BatchNorm1d(16),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv1d(16, 32, kernel_size=3, stride=1, padding=1, bias=True),\n",
        "            nn.BatchNorm1d(32),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        self.rnn = nn.RNN(32, hidden_dim, num_layers=n_layers, batch_first=True, bidirectional=bidirectional, nonlinearity='relu', dropout=dropout)   \n",
        "        multiplier = 2 if bidirectional else 1\n",
        "        self.fc = nn.Sequential(\n",
        "          nn.Linear(n_layers * hidden_dim * multiplier, output_size * 2),\n",
        "          nn.BatchNorm1d(output_size * 2),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(output_size * 2, output_size),\n",
        "      )\n",
        "    \n",
        "    def forward(self, x):\n",
        "      N = len(x)\n",
        "      out = x.permute(0, 2, 1)\n",
        "      out = self.cnn_encoder(out)\n",
        "      out = out.permute(0, 2, 1)\n",
        "      _, hidden = self.rnn(out)\n",
        "      hidden = hidden.permute(1,0,2).contiguous().view(N, -1)\n",
        "      hidden = self.fc(hidden)\n",
        "      return hidden\n",
        "\n",
        "class VanillaLSTM(nn.Module):\n",
        "  def __init__(self, input_size, output_size, hidden_dim, n_layers, dropout, bidirectional):\n",
        "        super(VanillaLSTM, self).__init__()\n",
        "\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.rnn = nn.LSTM(input_size, hidden_dim, num_layers=n_layers, batch_first=True, bidirectional=bidirectional, dropout=dropout)   \n",
        "        multiplier = 2 if bidirectional else 1\n",
        "        self.fc = nn.Sequential(\n",
        "          nn.Linear(n_layers * hidden_dim * multiplier, output_size * 2),\n",
        "          nn.BatchNorm1d(output_size * 2),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(output_size * 2, output_size),\n",
        "      )\n",
        "    \n",
        "  def forward(self, x):\n",
        "      N = len(x)\n",
        "      _, (hidden, _) = self.rnn(x)\n",
        "      hidden = hidden.permute(1,0,2).contiguous().view(N, -1)\n",
        "      hidden = self.fc(hidden)\n",
        "      return hidden\n",
        "\n",
        "class VanillaLSTMWithCNN(nn.Module):\n",
        "  def __init__(self, input_size, output_size, hidden_dim, n_layers, dropout, bidirectional):\n",
        "      super(VanillaLSTMWithCNN, self).__init__()\n",
        "\n",
        "      self.hidden_dim = hidden_dim\n",
        "      self.n_layers = n_layers\n",
        "\n",
        "      self.cnn_encoder = nn.Sequential(\n",
        "          nn.Conv1d(input_size, 8, kernel_size=3, stride=1, padding=1, bias=True),\n",
        "          nn.BatchNorm1d(8),\n",
        "          nn.ReLU(inplace=True),\n",
        "          nn.Conv1d(8, 16, kernel_size=3, stride=1, padding=1, bias=True),\n",
        "          nn.BatchNorm1d(16),\n",
        "          nn.ReLU(inplace=True),\n",
        "          nn.Conv1d(16, 32, kernel_size=3, stride=1, padding=1, bias=True),\n",
        "          nn.BatchNorm1d(32),\n",
        "          nn.ReLU(inplace=True),\n",
        "      )\n",
        "      self.rnn = nn.LSTM(32, hidden_dim, num_layers=n_layers, batch_first=True, bidirectional=bidirectional, dropout=dropout)   \n",
        "      multiplier = 2 if bidirectional else 1\n",
        "      self.fc = nn.Sequential(\n",
        "          nn.Linear(n_layers * hidden_dim * multiplier, output_size * 2),\n",
        "          nn.BatchNorm1d(output_size * 2),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(output_size * 2, output_size),\n",
        "      )\n",
        "  def forward(self, x):\n",
        "      N = len(x)\n",
        "      out = x.permute(0, 2, 1)\n",
        "      out = self.cnn_encoder(out)\n",
        "      out = out.permute(0, 2, 1)\n",
        "      _, (hidden, _) = self.rnn(out)\n",
        "      hidden = hidden.permute(1,0,2).contiguous().view(N, -1)\n",
        "      hidden = self.fc(hidden)\n",
        "      return hidden\n",
        "\n",
        "class VanillaGRU(nn.Module):\n",
        "  def __init__(self, input_size, output_size, hidden_dim, n_layers, dropout, bidirectional):\n",
        "        super(VanillaGRU, self).__init__()\n",
        "\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.rnn = nn.GRU(input_size, hidden_dim, num_layers=n_layers, batch_first=True, bidirectional=bidirectional, dropout=dropout)   \n",
        "        multiplier = 2 if bidirectional else 1\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(n_layers * hidden_dim * multiplier, output_size * 2),\n",
        "            nn.BatchNorm1d(output_size * 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(output_size * 2, output_size),\n",
        "        )\n",
        "    \n",
        "  def forward(self, x):\n",
        "      N = len(x)\n",
        "      _, hidden = self.rnn(x)\n",
        "      hidden = hidden.permute(1,0,2).contiguous().view(N, -1)\n",
        "      hidden = self.fc(hidden)\n",
        "      return hidden\n",
        "\n",
        "class VanillaGRUWithCNN(nn.Module):\n",
        "  def __init__(self, input_size, output_size, hidden_dim, n_layers, dropout, bidirectional):\n",
        "      super(VanillaGRUWithCNN, self).__init__()\n",
        "\n",
        "      self.hidden_dim = hidden_dim\n",
        "      self.n_layers = n_layers\n",
        "\n",
        "      self.cnn_encoder = nn.Sequential(\n",
        "          nn.Conv1d(input_size, 8, kernel_size=3, stride=1, padding=1, bias=True),\n",
        "          nn.BatchNorm1d(8),\n",
        "          nn.ReLU(inplace=True),\n",
        "          nn.Conv1d(8, 16, kernel_size=3, stride=1, padding=1, bias=True),\n",
        "          nn.BatchNorm1d(16),\n",
        "          nn.ReLU(inplace=True),\n",
        "          nn.Conv1d(16, 32, kernel_size=3, stride=1, padding=1, bias=True),\n",
        "          nn.BatchNorm1d(32),\n",
        "          nn.ReLU(inplace=True),\n",
        "      )\n",
        "      self.rnn = nn.GRU(32, hidden_dim, num_layers=n_layers, batch_first=True, bidirectional=bidirectional, dropout=dropout)   \n",
        "      multiplier = 2 if bidirectional else 1\n",
        "      self.fc = nn.Sequential(\n",
        "          nn.Linear(n_layers * hidden_dim * multiplier, output_size * 2),\n",
        "          nn.BatchNorm1d(output_size * 2),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(output_size * 2, output_size),\n",
        "      )\n",
        "  def forward(self, x):\n",
        "      N = len(x)\n",
        "      out = x.permute(0, 2, 1)\n",
        "      out = self.cnn_encoder(out)\n",
        "      out = out.permute(0, 2, 1)\n",
        "      _, hidden = self.rnn(out)\n",
        "      hidden = hidden.permute(1,0,2).contiguous().view(N, -1)\n",
        "      hidden = self.fc(hidden)\n",
        "      return hidden"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "id": "HL1XAG55_zi0"
      },
      "outputs": [],
      "source": [
        "# Util functions\n",
        "def draw(drawing):\n",
        "  for x,y in drawing:\n",
        "    plt.plot(x, y, marker='.')\n",
        "    plt.axis('off')\n",
        "  plt.gca().invert_yaxis()\n",
        "  plt.axis('equal')\n",
        "  plt.show()\n",
        "\n",
        "def one_hot(category):\n",
        "    arr = np.zeros(len(categories))\n",
        "    arr[category] = 1.0\n",
        "    return arr\n",
        "\n",
        "def train(model, X_train, y_train, epochs = 1, n_chunks = 1000, learning_rate = 0.003, weight_decay = 0, optimizer = 'SGD'):\n",
        "  print(\"Training model with epochs = {epochs}, learning rate = {lr}\\n\".format(epochs = epochs, lr = learning_rate))\n",
        "\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "  if optimizer == 'SGD':\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "  else:\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "  losses = []\n",
        "  for epoch in range(epochs):\n",
        "    running_loss = 0\n",
        "    images = torch.chunk(X_train, n_chunks)\n",
        "    labels = torch.chunk(y_train, n_chunks)\n",
        "\n",
        "    N = len(images)\n",
        "\n",
        "    for i in range(N):\n",
        "      optimizer.zero_grad()\n",
        "      output = model.forward(images[i])\n",
        "      loss = criterion(output, labels[i])\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      running_loss += loss.item()\n",
        "    losses.append(running_loss)\n",
        "    print(\"Epoch: {}/{}... \".format(epoch + 1, epochs), \"Loss: {:.4f}\".format(running_loss))\n",
        "  return losses\n",
        "  \n",
        "def get_prediction_probability_distribution(model, input):\n",
        "  with torch.no_grad():\n",
        "    logits = model.forward(input)\n",
        "  return torch.nn.functional.softmax(logits, dim=1)\n",
        "\n",
        "def get_labels(model, input):\n",
        "  probability_distribution = get_prediction_probability_distribution(model, input)\n",
        "  pred_np = probability_distribution.numpy()\n",
        "  N = len(pred_np)\n",
        "  pred_values = np.amax(pred_np, axis=1, keepdims=True)\n",
        "  pred_labels = np.array([np.where(pred_np[i, :] == pred_values[i, :])[0] for i in range(pred_np.shape[0])])\n",
        "  pred_labels = pred_labels.reshape(N, 1)\n",
        "  return pred_labels\n",
        "\n",
        "def evaluate_model(model, train, y_train, test, y_test):\n",
        "  train_pred_labels = get_labels(model, train)\n",
        "  test_pred_labels = get_labels(model, test)\n",
        "  y_train_label = np.argmax(y_train,axis=1)\n",
        "  y_test_label = np.argmax(y_test,axis=1)\n",
        "  accuracy_train = accuracy_score(y_train_label, train_pred_labels)\n",
        "  accuracy_test = accuracy_score(y_test_label, test_pred_labels)\n",
        "\n",
        "  print(\"Accuracy score for train set is {} \\n\".format(accuracy_train))\n",
        "  print(\"Accuracy score for test set is {} \\n\".format(accuracy_test))\n",
        "\n",
        "  return accuracy_train, accuracy_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZk-HG7UHl0i",
        "outputId": "3314f204-c2c8-42ba-a47e-d7b74a37d959"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU not available, CPU used\n"
          ]
        }
      ],
      "source": [
        "is_cuda = torch.cuda.is_available()\n",
        "if is_cuda:\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"GPU is available\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"GPU not available, CPU used\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "id": "CIYJXM5gr0yS"
      },
      "outputs": [],
      "source": [
        "categories = ['flower', 'apple', 'baseball', 'baskebtall', 'bird', 'book', 'bus', 'car', 'cat', 'dog']\n",
        "label_dict = {0:'flower', 1:'apple', 2:'baseball', 3:'baskebtall', 4:'bird',\n",
        "                      5:'book',6:'bus', 7:'car', 8:'cat', 9:'dog'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "id": "BaYtri9ztuZz"
      },
      "outputs": [],
      "source": [
        "# Data Hyperparameters\n",
        "test_set_split = 0.2\n",
        "MAX_SEQUENCE_LENGTH = 50\n",
        "EOS_TOKEN = None\n",
        "SEQUENCE_DIMENSION = 3 # [x, y, is_stroke_start]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {
        "id": "xV826495h6AH"
      },
      "outputs": [],
      "source": [
        "class RNNDataset(Dataset):\n",
        "    \"\"\"Face Landmarks dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, parsed_data, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            parsed_data (string): Path to the parsed_data.\n",
        "            transform (callable, optional): Optional transform to be applied\n",
        "                on a sample.\n",
        "        \"\"\"\n",
        "        with open(parsed_data, \"rb\") as fh:\n",
        "          self.file_pkg = pickle.load(fh)\n",
        "        # self.file_pkg = pd.read_pickle(parsed_data)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_pkg)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.file_pkg.iloc[idx].to_dict()\n",
        "\n",
        "def collate_fn(batch):\n",
        "  N = len(batch)\n",
        "  for i in range(N):\n",
        "    seq = batch[i]['drawing']\n",
        "    category = batch[i]['class']\n",
        "    input = np.zeros((MAX_SEQUENCE_LENGTH, SEQUENCE_DIMENSION), np.float32)\n",
        "    input[0:len(seq)] = seq[: MAX_SEQUENCE_LENGTH]\n",
        "    batch[i]['drawing'] = input\n",
        "  return batch\n",
        "\n",
        "# Returns a DataLoader\n",
        "# It already shuffles data as per sampler option\n",
        "def read_rnn_data(batch_size=4, num_workers=0):\n",
        "\ttransformed_dataset = RNNDataset('./rnn_parsed_data.pkl')\n",
        "\treturn DataLoader(transformed_dataset, batch_size, num_workers, collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "id": "ymoW7b8ir0_R"
      },
      "outputs": [],
      "source": [
        "# Load data for each category\n",
        "classes = defaultdict(lambda: [])\n",
        "\n",
        "# read data\n",
        "ENABLE_MULTI_BATCH = False\n",
        "READ_BATCH_SIZE = 50000\n",
        "loader = read_rnn_data(batch_size=READ_BATCH_SIZE, num_workers=10)\n",
        "for i_batch, sample_batched in enumerate(loader):\n",
        "  N = len(sample_batched)\n",
        "  for i in range(N):\n",
        "    seq = sample_batched[i]['drawing']\n",
        "    category = sample_batched[i]['class']\n",
        "    classes[category].append(seq)\n",
        "  if not ENABLE_MULTI_BATCH:\n",
        "    break\n",
        "\n",
        "# for category in categories:\n",
        "#     # data = pd.read_csv(\"../input/train_simplified/\" + category + \".csv\")\n",
        "#     data = [[[1,1,0],[2,2,0],[3,3,1],[4,4,1],[5,5,1]], [[1,1,0],[2,2,0],[3,3,1],[4,4,1],[5,5,1]]] # dummy\n",
        "#     N = len(data)\n",
        "#     input = np.zeros((N, MAX_SEQUENCE_LENGTH, SEQUENCE_DIMENSION), np.float32)\n",
        "#     for seq in data:\n",
        "#       length, _ = np.shape(seq)\n",
        "      \n",
        "#     for i in range(N):\n",
        "#       seq = data[i]\n",
        "#       input[i, 0:len(seq)] = seq[: MAX_SEQUENCE_LENGTH]\n",
        "#     classes[category] = input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {
        "id": "E3p82EKKr1bh"
      },
      "outputs": [],
      "source": [
        "# Process input output pairs\n",
        "X = []\n",
        "y = []\n",
        "for key, value in label_dict.items():\n",
        "    data_i = classes[value]\n",
        "    for data in data_i:\n",
        "      X.append(data)\n",
        "      y.append(one_hot(key))\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {
        "id": "KMGUWOpetJTY"
      },
      "outputs": [],
      "source": [
        "# Train test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {
        "id": "kPO02LPBt5s3"
      },
      "outputs": [],
      "source": [
        "# Convert data to torch\n",
        "X_train = torch.from_numpy(X_train).float()\n",
        "y_train = torch.from_numpy(y_train).float()\n",
        "X_test = torch.from_numpy(X_test).float()\n",
        "y_test = torch.from_numpy(y_test).float()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {
        "id": "R3vlUVDYr1hy"
      },
      "outputs": [],
      "source": [
        "# Model Hyperparameters\n",
        "input_size = 3\n",
        "hidden_sizes = 32\n",
        "output_size = len(categories)\n",
        "n_layers = 4\n",
        "dropout = 0.1\n",
        "weight_decay = 0.0\n",
        "epochs = 20\n",
        "n_chunks = 200\n",
        "learning_rate = 0.01\n",
        "optimizer = 'SGD'\n",
        "bidirectional= True\n",
        "\n",
        "# Define model\n",
        "model = VanillaRNN(input_size, output_size, hidden_sizes, n_layers, dropout, bidirectional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zk-40Tkgr1tn",
        "outputId": "a284e774-7d30-434c-ff75-53d2baf60355"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model with epochs = 20, learning rate = 0.01\n",
            "\n",
            "Epoch: 1/20...  Loss: 418.0599\n",
            "Epoch: 2/20...  Loss: 379.1228\n",
            "Epoch: 3/20...  Loss: 342.6028\n",
            "Epoch: 4/20...  Loss: 300.9908\n",
            "Epoch: 5/20...  Loss: 260.3591\n",
            "Epoch: 6/20...  Loss: 239.5989\n",
            "Epoch: 7/20...  Loss: 224.8894\n",
            "Epoch: 8/20...  Loss: 213.3865\n",
            "Epoch: 9/20...  Loss: 203.3009\n",
            "Epoch: 10/20...  Loss: 194.1901\n",
            "Epoch: 11/20...  Loss: 184.9012\n",
            "Epoch: 12/20...  Loss: 177.2083\n",
            "Epoch: 13/20...  Loss: 171.6928\n",
            "Epoch: 14/20...  Loss: 167.0449\n",
            "Epoch: 15/20...  Loss: 163.3282\n",
            "Epoch: 16/20...  Loss: 160.6098\n",
            "Epoch: 17/20...  Loss: 155.3722\n",
            "Epoch: 18/20...  Loss: 151.7895\n",
            "Epoch: 19/20...  Loss: 148.3414\n",
            "Epoch: 20/20...  Loss: 145.9939\n"
          ]
        }
      ],
      "source": [
        "# Train model\n",
        "loss = train(model, X_train, y_train, epochs=epochs, learning_rate = learning_rate, weight_decay = weight_decay, n_chunks = n_chunks, optimizer = optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0OHdTmNUr1yo",
        "outputId": "b715b468-7223-463c-943a-97658eef0187"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy score for train set is 0.7495029821073559 \n",
            "\n",
            "Accuracy score for test set is 0.7438983986747654 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# get accuracy\n",
        "accuracy_train, accuracy_test = evaluate_model(model, X_train, y_train, X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {
        "id": "JMq0iRvQzWiZ"
      },
      "outputs": [],
      "source": [
        "# Model Hyperparameters\n",
        "input_size = 3\n",
        "hidden_sizes = 32\n",
        "output_size = len(categories)\n",
        "n_layers = 4\n",
        "dropout = 0.0\n",
        "weight_decay = 0.0\n",
        "epochs = 20\n",
        "n_chunks = 200\n",
        "learning_rate = 0.01\n",
        "optimizer = 'SGD'\n",
        "bidirectional = True\n",
        "\n",
        "# Define model\n",
        "model = VanillaRNNWithCNN(input_size, output_size, hidden_sizes, n_layers, dropout, bidirectional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6UFf7tHGzW46",
        "outputId": "15fe0c74-7cb4-451d-8a04-79fb73e366bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model with epochs = 20, learning rate = 0.01\n",
            "\n",
            "Epoch: 1/20...  Loss: 410.0557\n",
            "Epoch: 2/20...  Loss: 342.0093\n",
            "Epoch: 3/20...  Loss: 293.6451\n",
            "Epoch: 4/20...  Loss: 263.2213\n",
            "Epoch: 5/20...  Loss: 240.8271\n",
            "Epoch: 6/20...  Loss: 223.5692\n",
            "Epoch: 7/20...  Loss: 208.3804\n",
            "Epoch: 8/20...  Loss: 194.0322\n",
            "Epoch: 9/20...  Loss: 173.3156\n",
            "Epoch: 10/20...  Loss: 155.9448\n",
            "Epoch: 11/20...  Loss: 146.1771\n",
            "Epoch: 12/20...  Loss: 137.2430\n",
            "Epoch: 13/20...  Loss: 130.6573\n",
            "Epoch: 14/20...  Loss: 125.3805\n",
            "Epoch: 15/20...  Loss: 120.8006\n",
            "Epoch: 16/20...  Loss: 116.2722\n",
            "Epoch: 17/20...  Loss: 112.7427\n",
            "Epoch: 18/20...  Loss: 109.5250\n",
            "Epoch: 19/20...  Loss: 106.4043\n",
            "Epoch: 20/20...  Loss: 103.3892\n"
          ]
        }
      ],
      "source": [
        "# Train model\n",
        "loss = train(model, X_train, y_train, epochs=epochs, learning_rate = learning_rate, weight_decay = weight_decay, n_chunks = n_chunks, optimizer = optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pOx_DbiNzXK3",
        "outputId": "19cf52e8-376e-4efb-dfe7-304653c907ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy score for train set is 0.8311795891318754 \n",
            "\n",
            "Accuracy score for test set is 0.8230811706239647 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# get accuracy\n",
        "accuracy_train, accuracy_test = evaluate_model(model, X_train, y_train, X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {
        "id": "vxIRVpFlr13B"
      },
      "outputs": [],
      "source": [
        "# Model Hyperparameters\n",
        "input_size = 3\n",
        "hidden_sizes = 32\n",
        "output_size = len(categories)\n",
        "n_layers = 3\n",
        "dropout = 0.0\n",
        "weight_decay = 0.0\n",
        "epochs = 20\n",
        "n_chunks = 100\n",
        "learning_rate = 0.1\n",
        "optimizer = 'SGD'\n",
        "bidirectional = True\n",
        "\n",
        "# Define model\n",
        "model = VanillaLSTM(input_size, output_size, hidden_sizes, n_layers, dropout, bidirectional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZHUG-deVr16v",
        "outputId": "b1c8df90-090e-4c1a-9c07-1b816fb2237a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model with epochs = 20, learning rate = 0.1\n",
            "\n",
            "Epoch: 1/20...  Loss: 170.6856\n",
            "Epoch: 2/20...  Loss: 130.0653\n",
            "Epoch: 3/20...  Loss: 105.9991\n",
            "Epoch: 4/20...  Loss: 86.3209\n",
            "Epoch: 5/20...  Loss: 75.8939\n",
            "Epoch: 6/20...  Loss: 69.7249\n",
            "Epoch: 7/20...  Loss: 64.6209\n",
            "Epoch: 8/20...  Loss: 60.9655\n",
            "Epoch: 9/20...  Loss: 56.2669\n",
            "Epoch: 10/20...  Loss: 53.0282\n",
            "Epoch: 11/20...  Loss: 49.8089\n",
            "Epoch: 12/20...  Loss: 47.4998\n",
            "Epoch: 13/20...  Loss: 45.5068\n",
            "Epoch: 14/20...  Loss: 44.0345\n",
            "Epoch: 15/20...  Loss: 41.9154\n",
            "Epoch: 16/20...  Loss: 40.4857\n",
            "Epoch: 17/20...  Loss: 38.6409\n",
            "Epoch: 18/20...  Loss: 38.2479\n",
            "Epoch: 19/20...  Loss: 36.3907\n",
            "Epoch: 20/20...  Loss: 35.8636\n"
          ]
        }
      ],
      "source": [
        "# Train model\n",
        "loss = train(model, X_train, y_train, epochs=epochs, learning_rate = learning_rate, weight_decay = weight_decay, n_chunks = n_chunks, optimizer = optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uosOihTQr1-e",
        "outputId": "9583e2f7-a695-4b79-ecbc-7dd4896b4e84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy score for train set is 0.8766291142036668 \n",
            "\n",
            "Accuracy score for test set is 0.8685808945334069 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# get accuracy\n",
        "accuracy_train, accuracy_test = evaluate_model(model, X_train, y_train, X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {
        "id": "CblLdDlMr2B7"
      },
      "outputs": [],
      "source": [
        "# Model Hyperparameters\n",
        "input_size = 3\n",
        "hidden_sizes = 32\n",
        "output_size = len(categories)\n",
        "n_layers = 4\n",
        "dropout = 0.05\n",
        "weight_decay = 0.0\n",
        "epochs = 10\n",
        "n_chunks = 200\n",
        "learning_rate = 0.1\n",
        "optimizer = 'SGD'\n",
        "bidirectional = True\n",
        "\n",
        "# Define model\n",
        "model = VanillaLSTMWithCNN(input_size, output_size, hidden_sizes, n_layers, dropout, bidirectional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-yyhNitr2Fl",
        "outputId": "2fb6adb4-a16b-4802-b628-7de1fc6f6af6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model with epochs = 10, learning rate = 0.1\n",
            "\n",
            "Epoch: 1/10...  Loss: 264.0151\n",
            "Epoch: 2/10...  Loss: 158.5255\n",
            "Epoch: 3/10...  Loss: 124.6494\n",
            "Epoch: 4/10...  Loss: 105.8081\n",
            "Epoch: 5/10...  Loss: 91.6933\n",
            "Epoch: 6/10...  Loss: 81.3721\n",
            "Epoch: 7/10...  Loss: 74.9141\n",
            "Epoch: 8/10...  Loss: 70.0046\n",
            "Epoch: 9/10...  Loss: 66.2039\n",
            "Epoch: 10/10...  Loss: 63.0440\n"
          ]
        }
      ],
      "source": [
        "# Train model\n",
        "loss = train(model, X_train, y_train, epochs=epochs, learning_rate = learning_rate, weight_decay = weight_decay, n_chunks = n_chunks, optimizer = optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQHOtzsur2Ja",
        "outputId": "003a9b34-4c91-4a0f-823d-b6f6c179f8f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy score for train set is 0.8926993593991606 \n",
            "\n",
            "Accuracy score for test set is 0.8779679734953064 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# get accuracy\n",
        "accuracy_train, accuracy_test = evaluate_model(model, X_train, y_train, X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {
        "id": "BRUH0RD8r2Ni"
      },
      "outputs": [],
      "source": [
        "# Model Hyperparameters\n",
        "input_size = 3\n",
        "hidden_sizes = 32\n",
        "output_size = len(categories)\n",
        "n_layers = 3\n",
        "dropout = 0.0\n",
        "weight_decay = 0.0\n",
        "epochs = 20\n",
        "n_chunks = 100\n",
        "learning_rate = 0.1\n",
        "optimizer = 'SGD'\n",
        "bidirectional = True\n",
        "\n",
        "# Define model\n",
        "model = VanillaGRU(input_size, output_size, hidden_sizes, n_layers, dropout, bidirectional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ND78Q15CnvDv",
        "outputId": "546af787-cace-4da4-f769-ab0e28357a43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model with epochs = 20, learning rate = 0.1\n",
            "\n",
            "Epoch: 1/20...  Loss: 167.9943\n",
            "Epoch: 2/20...  Loss: 127.0240\n",
            "Epoch: 3/20...  Loss: 107.5064\n",
            "Epoch: 4/20...  Loss: 90.1924\n",
            "Epoch: 5/20...  Loss: 75.5425\n",
            "Epoch: 6/20...  Loss: 65.4412\n",
            "Epoch: 7/20...  Loss: 58.8963\n",
            "Epoch: 8/20...  Loss: 53.4405\n",
            "Epoch: 9/20...  Loss: 49.5826\n",
            "Epoch: 10/20...  Loss: 46.0944\n",
            "Epoch: 11/20...  Loss: 43.0165\n",
            "Epoch: 12/20...  Loss: 40.9014\n",
            "Epoch: 13/20...  Loss: 38.8624\n",
            "Epoch: 14/20...  Loss: 37.2167\n",
            "Epoch: 15/20...  Loss: 35.9022\n",
            "Epoch: 16/20...  Loss: 34.6183\n",
            "Epoch: 17/20...  Loss: 33.4115\n",
            "Epoch: 18/20...  Loss: 32.4174\n",
            "Epoch: 19/20...  Loss: 31.4944\n",
            "Epoch: 20/20...  Loss: 30.7709\n"
          ]
        }
      ],
      "source": [
        "# Train model\n",
        "loss = train(model, X_train, y_train, epochs=epochs, learning_rate = learning_rate, weight_decay = weight_decay, n_chunks = n_chunks, optimizer = optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 168,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kocFtbQQoeW4",
        "outputId": "3ab70b53-ebf9-4bfe-a5ce-37b5ffcc5cef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy score for train set is 0.8924508504528386 \n",
            "\n",
            "Accuracy score for test set is 0.8742131419105467 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# get accuracy\n",
        "accuracy_train, accuracy_test = evaluate_model(model, X_train, y_train, X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 169,
      "metadata": {
        "id": "CHkuZHQuowOL"
      },
      "outputs": [],
      "source": [
        "# Model Hyperparameters\n",
        "input_size = 3\n",
        "hidden_sizes = 32\n",
        "output_size = len(categories)\n",
        "n_layers = 3\n",
        "dropout = 0.0\n",
        "weight_decay = 0.0\n",
        "epochs = 20\n",
        "n_chunks = 100\n",
        "learning_rate = 0.1\n",
        "optimizer = 'SGD'\n",
        "bidirectional = True\n",
        "\n",
        "# Define model\n",
        "model = VanillaGRUWithCNN(input_size, output_size, hidden_sizes, n_layers, dropout, bidirectional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 170,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o2CDIurFfTpP",
        "outputId": "4fcfc18c-9b63-4fbc-efba-5284d91a20d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model with epochs = 20, learning rate = 0.1\n",
            "\n",
            "Epoch: 1/20...  Loss: 152.2372\n",
            "Epoch: 2/20...  Loss: 99.7676\n",
            "Epoch: 3/20...  Loss: 73.7976\n",
            "Epoch: 4/20...  Loss: 60.9642\n",
            "Epoch: 5/20...  Loss: 53.1641\n",
            "Epoch: 6/20...  Loss: 47.5618\n",
            "Epoch: 7/20...  Loss: 43.0744\n",
            "Epoch: 8/20...  Loss: 39.5680\n",
            "Epoch: 9/20...  Loss: 36.7835\n",
            "Epoch: 10/20...  Loss: 34.5181\n",
            "Epoch: 11/20...  Loss: 32.6944\n",
            "Epoch: 12/20...  Loss: 31.1433\n",
            "Epoch: 13/20...  Loss: 29.8523\n",
            "Epoch: 14/20...  Loss: 28.7488\n",
            "Epoch: 15/20...  Loss: 27.7439\n",
            "Epoch: 16/20...  Loss: 26.8560\n",
            "Epoch: 17/20...  Loss: 26.1235\n",
            "Epoch: 18/20...  Loss: 25.4106\n",
            "Epoch: 19/20...  Loss: 24.7633\n",
            "Epoch: 20/20...  Loss: 24.1525\n"
          ]
        }
      ],
      "source": [
        "# Train model\n",
        "loss = train(model, X_train, y_train, epochs=epochs, learning_rate = learning_rate, weight_decay = weight_decay, n_chunks = n_chunks, optimizer = optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 171,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BS5OgC9efcju",
        "outputId": "ca8fdcc2-b7e4-413c-8f39-5e8f38a73225"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy score for train set is 0.9181577203445991 \n",
            "\n",
            "Accuracy score for test set is 0.8965212589729431 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# get accuracy\n",
        "accuracy_train, accuracy_test = evaluate_model(model, X_train, y_train, X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 172,
      "metadata": {
        "id": "MELFD3jyfUIj"
      },
      "outputs": [],
      "source": [
        "# drawing = [[[121,107,45,17,1,0,4,21,58,118,173,197,209,224,244,254,254,209,164,124],[47,57,56,75,93,114,123,140,162,187,196,187,177,164,136,115,101,83,71,43]],[[123,126],[43,0]]]\n",
        "# draw(drawing)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "t1rLMmfMDMch"
      },
      "execution_count": 172,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "CS7643 Final Project RNN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}